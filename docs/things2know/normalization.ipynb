{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things you should know about normalization\n",
    "\n",
    "## Start from a common problem in NN training\n",
    "\n",
    "Data may have different var and mean across batches, which might lead to training problems:\n",
    "1. the var and mean diversity are too much between batches, which leads to multi-scale features, thus parameters\n",
    "2. the multi-scale parameter adds difficulty to grad descent optimization\n",
    "\n",
    "## method for batch normalization\n",
    "\n",
    "suppose we have data $Z_{ij}$ of shape (i: feature_dim, j: batch_size)\n",
    "\n",
    "to normalize on the second dimensiton: $Z'_{ij} = \\frac{Z_{ij}-\\sum_j Z_{ij}/N}{\\sqrt{\\sum_j(Z_{ij}-\\sum_j Z_{ij}/N)^2/N + \\epsilon}}$\n",
    "\n",
    "This is the same as\n",
    "\n",
    "$Z'_{ij} = \\frac{Z_{ij}-\\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$, where $\\mu_i = \\sum_j Z_{ij}/N$, $\\sigma_i^2 = \\sum_j(Z_{ij}-\\mu_i)^2/N$\n",
    "\n",
    "To still encourage var and mean adjust,\n",
    "\n",
    "$Z''_{ij} = G_i Z'_{ij} + B_i$\n",
    "\n",
    "Rewrite the function $Z''_{ij} = f_{G_i, B_i}(Z_{ij})$ as batch normalization, it has a number of feature params, that is to say, each dim of feature the normalization is dependent.\n",
    "\n",
    "## Where to add batch norm in NN?\n",
    "\n",
    "Usually, batch normalization is added before the activation function, after the linear layer.\n",
    "\n",
    "$y = Activation(BatchNorm(WX))$\n",
    "\n",
    "There is an additional normalization parameter $G_i$, $B_i$ to train. This part is named normalized activation according to the original paper.\n",
    "\n",
    "## does it work?\n",
    "\n",
    "Normalized data makes gradient descent learning easier, take this view for each layer, we would like to normalize all the layers by mean and var of the whole dataset during training.\n",
    "\n",
    "But, it is never possible to calculate mean and var for all the data, since each update requires a full loop of the dataset. The covariance adds difficulty to normalization since it might not be invertible. Thus, BN makes two assumptions: 1. each dimension of data is independent. 2. the batch data is a good approximation of full data on mean and var.\n",
    "\n",
    "Suppose we want to predict a of data [a, a+1, a+2] given the seq.\n",
    "\n",
    "This makes a bad case for applying batch normalization because the mean and var of data are not the same and each dimension is not dependent.\n",
    "\n",
    "## a test experiment: \n",
    "\n",
    "To apply batch normalization to the convolutional neural network. Suppose we have a p*q feature map after convolution, then we have p*q channels and they are normalized separately.\n",
    "\n",
    "\n",
    "There is a codebase that does a really similar thing: https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Lesson.ipynb\n",
    "\n",
    "## reference\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPW-mOq2XAYX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
