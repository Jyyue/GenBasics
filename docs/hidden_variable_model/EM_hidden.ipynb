{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is optimization of hidden variable? Why do we need it? -- start from simple gaussian\n",
    "\n",
    "## benefit of modeling hidden\n",
    "\n",
    "The answer is simple: compared with fully connected model of observable, model with hidden is simpler, wich less parameters.\n",
    "\n",
    "## estimate hidden distribution: why and how\n",
    "\n",
    "Think of a hidden variable model, h --> x.\n",
    "\n",
    "$$p(x|\\pi, \\theta) = \\sum_i \\pi_i p(x|\\theta_{h_i})$$\n",
    "\n",
    "Now we get a sample from data, mark as x_0. You have no idea the corresponding hidden. That is, although we have this model, we have no idea on where x is from! This is not good.(why?)\n",
    "\n",
    "How to solve this problem? To know where x is from, we need p(h|x).\n",
    "\n",
    "We then realise that this can be calculated by Bayes rule, $$p(h|x) = \\frac{p(h,x)}{p(x)} = \\frac{p(x|h_i)p(h_i)}{\\sum_i p(x|h_i)p(h_i)}$$\n",
    "\n",
    "Here $p(h_i)$ is the prior of gaussian distribution, as \\mu_i and \\Sigma_i\\in R^{dxd}. x\\in R^d. i\\in {1,...,k}, represent multiple gaussian distributions are added to get the final distribution.\n",
    "\n",
    "Then we realize something: if we know p(x|h), which is the variable $\\pi$ of model distribution, and the prior, which is the mean $\\mu$ and variance $\\Sigma$ of gaussian distribution, the posterior can be estimated by\n",
    "\n",
    "$$\\pi_i = p(\\theta_i|x)/\\sum_i p(\\theta_i|x)$$ \n",
    "-> the higher the possibility of p(h_i|x), the more likely x is from distribution of h.\n",
    "\n",
    "$$\\mu_i = $$\n",
    "\n",
    "What is the benefit? We can update prior by new estimated posterior, and get the new p(h|x). Continue this, we have the converge result.\n",
    "\n",
    "Full process:\n",
    "build model $p(x|\\pi,\\mu) = $ x conitionals on several gaussian (multi-dim gaussian with different weighting parameter)\n",
    "get data sample $data\\in x^{k,n}$\n",
    "get p(h|x_j) = p(x|\\pi,\\mu) \\pi\\mu is prior. => possibility of x from certain gaussian compoment\n",
    "average on all x to get possibility of data from certain gaussian. => update pi of each gaussian\n",
    "average on all data to get the vec mean, each gaussian get the part of its distribution mean_i => update \\mu of each gaussian\n",
    "the new mean and old mean on each data averaged, deduce and square to get variance => update \\Sigma of each gaussian\n",
    "\n",
    "\n",
    "The following is an example of learning Bayes Neural network.\n",
    "\n",
    "Problem set up: candy from bags\n",
    "\n",
    "you have two bags, each\n",
    "\n",
    "The following is an example of this on SwissRoll dataset.\n",
    "\n",
    "# reference\n",
    "\n",
    "http://vision.psych.umn.edu/users/schrater/schrater_lab/courses/AI2/em1.pdf\n",
    "\n",
    "http://www.ai.mit.edu/courses/6.825/fall02/pdf/6.825-lecture-18.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def initialize_parameters(k, d):\n",
    "    \"\"\"\n",
    "    Initialize GMM parameters.\n",
    "    k: Number of components\n",
    "    d: Dimension of the data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    pi = np.ones(k) / k  # Equal mixture weights\n",
    "    mu = np.random.rand(k, d) * 10  # Random means in the range [0, 10)\n",
    "    sigma = np.array([np.eye(d) for _ in range(k)])  # Identity covariance matrices\n",
    "    return pi, mu, sigma\n",
    "\n",
    "def e_step(data, pi, mu, sigma, k):\n",
    "    \"\"\"\n",
    "    E-step: Compute the posterior probabilities p(h|x_i)\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    gamma = np.zeros((n, k))\n",
    "    for h in range(k):\n",
    "        # Multivariate normal probability density\n",
    "        gamma[:, h] = pi[h] * multivariate_normal.pdf(data, mean=mu[h], cov=sigma[h])\n",
    "    gamma /= gamma.sum(axis=1, keepdims=True)  # Normalize across components\n",
    "    return gamma\n",
    "\n",
    "def m_step(data, gamma, k):\n",
    "    \"\"\"\n",
    "    M-step: Update the parameters {π_h, μ_h, Σ_h} based on gamma\n",
    "    \"\"\"\n",
    "    n, d = data.shape\n",
    "    n_h = gamma.sum(axis=0)  # Effective number of points in each component\n",
    "    pi = n_h / n  # Update mixture weights\n",
    "    mu = np.dot(gamma.T, data) / n_h[:, np.newaxis]  # Update means\n",
    "    sigma = np.zeros((k, d, d))\n",
    "    for h in range(k):\n",
    "        diff = data - mu[h]  # x_i - μ_h\n",
    "        sigma[h] = np.dot(gamma[:, h] * diff.T, diff) / n_h[h]  # Update covariances\n",
    "    return pi, mu, sigma\n",
    "\n",
    "def log_likelihood(data, pi, mu, sigma, k):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of the data given the model parameters\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    likelihood = 0\n",
    "    for h in range(k):\n",
    "        likelihood += pi[h] * multivariate_normal.pdf(data, mean=mu[h], cov=sigma[h])\n",
    "    return np.sum(np.log(likelihood))\n",
    "\n",
    "def em_algorithm(data, k, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    EM algorithm for Gaussian Mixture Models\n",
    "    \"\"\"\n",
    "    n, d = data.shape\n",
    "    pi, mu, sigma = initialize_parameters(k, d)  # Initialize parameters\n",
    "    prev_likelihood = None\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: Compute responsibilities\n",
    "        gamma = e_step(data, pi, mu, sigma, k)\n",
    "\n",
    "        # M-step: Update parameters\n",
    "        pi, mu, sigma = m_step(data, gamma, k)\n",
    "\n",
    "        # Compute log-likelihood\n",
    "        likelihood = log_likelihood(data, pi, mu, sigma, k)\n",
    "        print(f\"Iteration {iteration + 1}, Log-Likelihood: {likelihood}\")\n",
    "\n",
    "        # Check for convergence\n",
    "        if prev_likelihood is not None and abs(likelihood - prev_likelihood) < tol:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "        prev_likelihood = likelihood\n",
    "\n",
    "    return pi, mu, sigma\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "np.random.seed(42)\n",
    "data1 = np.random.multivariate_normal([2, 2], [[1, 0.5], [0.5, 1]], size=100)\n",
    "data2 = np.random.multivariate_normal([8, 8], [[1, -0.5], [-0.5, 1]], size=100)\n",
    "data = np.vstack([data1, data2])\n",
    "\n",
    "# Run the EM algorithm\n",
    "k = 2  # Number of components\n",
    "pi, mu, sigma = em_algorithm(data, k)\n",
    "\n",
    "print(\"Final Parameters:\")\n",
    "print(\"Mixture Weights (pi):\", pi)\n",
    "print(\"Means (mu):\", mu)\n",
    "print(\"Covariances (sigma):\", sigma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPW-mOq2XAYX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
