{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to make deep neural network stable?\n",
    "\n",
    "## xavier initialization\n",
    "\n",
    "think of MLP\n",
    "\n",
    "$y = Wx + b$\n",
    "\n",
    "if $x \\sim U(0, \\mathcal{1}_{d_{in}})$, $W \\sim U(0, \\mathcal{1}_{d_{out}\\times d_{in}})$, then $y \\sim U(0, d_{in} \\mathcal{1}_{d_{out}})$\n",
    "\n",
    "the variance will explore as the neural network goes deeper.\n",
    "\n",
    "In [paper](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf0), a weight initialization method is proposed. Instead of initial with Uniform distribution $U[-1, 1]$, we should initial from uniform distribution with $U[-1/\\sqrt{d_{in}}, 1/\\sqrt{d_{in}}]$.\n",
    "\n",
    "This keeps the variance of neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the idea of xavier initializaiton\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "d_in = 10\n",
    "d_out = 10\n",
    "\n",
    "x = torch.randn(1000, d_in)\n",
    "W1 = (torch.rand(d_in, d_out) - 0.5)*2*math.sqrt(3)\n",
    "W2 = W1 / math.sqrt(d_in)\n",
    "y1 = torch.einsum('ij,kj->ik', x, W1)\n",
    "y2 = torch.einsum('ij,kj->ik', x, W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0002) tensor(0.9782) tensor(9.6752)\n",
      "tensor(1.0002) tensor(0.0978) tensor(0.9675)\n"
     ]
    }
   ],
   "source": [
    "print(x.var(), W1.var(), y1.var())\n",
    "print(x.var(), W2.var(), y2.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数导致方差的变化\n",
    "\n",
    "若输出是正态分布，方差变化为原分布的0.341\n",
    "\n",
    "若输入是平均分布，方差变化为0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var input 0.9954899937768414\n",
      "var before relu 1.9777306782911959\n",
      "var after relu 0.6738203141509091\n",
      "var before relu 1.9375079691397927\n",
      "var after relu 0.6737599491212752\n",
      "var before relu 1.9557270470001271\n",
      "var after relu 0.6171111721672213\n",
      "var before relu 1.7624774460636328\n",
      "var after relu 0.5390921383593544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# He初始化（适用于ReLU）\n",
    "def he_init(dim_in, dim_out):\n",
    "    return np.random.randn(dim_in, dim_out) * np.sqrt(2.0 / dim_in)\n",
    "\n",
    "# ReLU激活函数\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# MLP层\n",
    "class MLPLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = he_init(input_dim, output_dim)  # He初始化权重\n",
    "        self.b = np.zeros(output_dim)            # 偏置初始化为0\n",
    "\n",
    "    def forward(self, x):\n",
    "        rst = np.dot(x, self.W) + self.b\n",
    "        return rst\n",
    "\n",
    "# 标准的MLP网络\n",
    "class MLP:\n",
    "    def __init__(self, layer_dims):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layer = MLPLayer(layer_dims[i], layer_dims[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "            print('var before relu', x.var())\n",
    "            x = relu(x)  # 除最后一层外，每层后接ReLU\n",
    "            # 丢掉小于零的项\n",
    "            \n",
    "            print('var after relu', x.var())\n",
    "        return x\n",
    "\n",
    "# 设置网络结构和输入数据\n",
    "input_dim = 100\n",
    "hidden_dims = [100, 100, 100]  # 3个隐藏层，每层100个神经元\n",
    "output_dim = 100\n",
    "layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "# 初始化MLP\n",
    "mlp = MLP(layer_dims)\n",
    "\n",
    "# 生成输入数据 (均值为0，方差为1)\n",
    "num_samples = 1000\n",
    "x = np.random.randn(num_samples, input_dim)\n",
    "print('var input', x.var())\n",
    "# 前向传播\n",
    "output = mlp.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU前的方差: 0.9993980444129914\n",
      "ReLU后的方差: 0.34012237903980674\n",
      "方差变化比例: 0.3403272409238921\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成正态分布的随机数据 (均值为0，方差为1)\n",
    "num_samples = 1000000\n",
    "z = np.random.randn(num_samples)\n",
    "\n",
    "# 计算ReLU前的方差\n",
    "var_before = np.var(z)\n",
    "\n",
    "# 应用ReLU\n",
    "relu_z = np.maximum(0, z)\n",
    "\n",
    "# 计算ReLU后的方差\n",
    "var_after = np.var(relu_z)\n",
    "\n",
    "print(\"ReLU前的方差:\", var_before)\n",
    "print(\"ReLU后的方差:\", var_after)\n",
    "print(\"方差变化比例:\", var_after / var_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU前的方差: tensor(0.9988)\n",
      "ReLU后的方差: tensor(0.3120)\n",
      "方差变化比例: tensor(0.3124)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# 生成正态分布的随机数据 (均值为0，方差为1)\n",
    "num_samples = 1000000\n",
    "z = 2*math.sqrt(3)*(torch.rand(num_samples) - 0.5)\n",
    "\n",
    "# 计算ReLU前的方差\n",
    "var_before = z.var()\n",
    "\n",
    "# 应用ReLU\n",
    "relu_z = torch.relu(z)\n",
    "\n",
    "# 计算ReLU后的方差\n",
    "var_after = relu_z.var()\n",
    "\n",
    "print(\"ReLU前的方差:\", var_before)\n",
    "print(\"ReLU后的方差:\", var_after)\n",
    "print(\"方差变化比例:\", var_after / var_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "径向函数的norm测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input r mean: 4.864406698692232\n",
      "input r variance: 0.4202306512605468\n",
      "RBF Embedding Means: [0.01020935 0.00686296 0.00401549 0.00819955 0.0145136  0.01811526\n",
      " 0.02413803 0.10123646 0.52405653 0.94740698]\n",
      "RBF Embedding Variances: [0.00977696 0.00412055 0.00153939 0.00546363 0.01064526 0.01264845\n",
      " 0.01375641 0.01292222 0.01195515 0.04679431]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gaussian_rbf_embedding(r, centers, beta):\n",
    "    \"\"\"\n",
    "    计算 RBF embedding\n",
    "    :param r: 输入距离数组 (N,)\n",
    "    :param centers: RBF 中心值数组 (M,)\n",
    "    :param beta: RBF 宽度参数 (标量或数组 (M,))\n",
    "    :return: (N, M) 的嵌入矩阵\n",
    "    \"\"\"\n",
    "    r = r[:, np.newaxis]  # 变为 (N, 1) 便于广播\n",
    "    return np.exp(-beta * (r - centers) ** 2)\n",
    "\n",
    "# ====== 配置参数 ======\n",
    "num_samples = 1000  # 采样点数\n",
    "num_rbfs = 10       # RBF 维度\n",
    "r_min, r_max = 0, 5 # 取值范围\n",
    "beta = 2.0          # RBF 宽度\n",
    "\n",
    "# 生成输入 r 的分布，例如正态分布或均匀分布\n",
    "r_samples = np.random.normal(loc=2.5, scale=1.0, size=num_samples)  # 正态分布\n",
    "r_samples = np.clip(r_samples, r_min, r_max)  # 限制范围\n",
    "\n",
    "# 设定 RBF 中心点（均匀分布在 [r_min, r_max]）\n",
    "centers = np.linspace(r_min, r_max, num_rbfs)\n",
    "\n",
    "# 计算 RBF embedding\n",
    "embeddings = gaussian_rbf_embedding(r_samples, centers, beta)\n",
    "\n",
    "# 计算每个 RBF 维度的均值和方差\n",
    "embedding_means = np.mean(embeddings, axis=0)\n",
    "embedding_vars = np.var(embeddings, axis=0)\n",
    "'''\n",
    "# ====== 结果可视化 ======\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 原始 r 分布\n",
    "axes[0].hist(r_samples, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title(\"Input $r$ Distribution\")\n",
    "axes[0].set_xlabel(\"$r$\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Embedding 分布\n",
    "for i in range(num_rbfs):\n",
    "    axes[1].hist(embeddings[:, i], bins=30, alpha=0.5, label=f'RBF {i+1}')\n",
    "\n",
    "axes[1].set_title(\"RBF Embedding Distributions\")\n",
    "axes[1].set_xlabel(\"Embedding Value\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''\n",
    "# 打印 embedding 统计信息\n",
    "print(\"input r mean:\", r_samples.mean())\n",
    "print(\"input r variance:\", r_samples.var())\n",
    "print(\"RBF Embedding Means:\", embedding_means)\n",
    "print(\"RBF Embedding Variances:\", embedding_vars)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPW-mOq2XAYX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
