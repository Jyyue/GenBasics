{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things you should know about nromalization\n",
    "\n",
    "## Start from a common problem in NN training\n",
    "\n",
    "Data may have different var and mean across batch, which might lead to training problem:\n",
    "1. the var and mean diverse too much between batch, which lead to multi-scale feature, thus parameters\n",
    "2. the multi-scale parameter add difficulty to grad dscent optimization\n",
    "\n",
    "## method for batch normalization\n",
    "\n",
    "suppose we have data $Z_{ij}$ of shape (i: feature_dim, j: batch_size)\n",
    "\n",
    "to normalize on the second dimensiton: $Z'_{ij} = \\frac{Z_{ij}-\\sum_j Z_{ij}/N}{\\sqrt{\\sum_j(Z_{ij}-\\sum_j Z_{ij}/N)^2/N + \\epsilon}}$\n",
    "\n",
    "this is the same as\n",
    "\n",
    "$Z'_{ij} = \\frac{Z_{ij}-\\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$, where $\\mu_i = \\sum_j Z_{ij}/N$, $\\sigma_i^2 = \\sum_j(Z_{ij}-\\mu_i)^2/N$\n",
    "\n",
    "To still encourage var and mean adjust,\n",
    "\n",
    "$Z''_{ij} = G_i Z'_{ij} + B_i$\n",
    "\n",
    "Rewrite the function $Z''_{ij} = f_{G_i, B_i}(Z_{ij})$ as batch normalization, it has number of feature params, that is to say each dim of feature the normalization is dependent.\n",
    "\n",
    "## where to add batch norm in NN?\n",
    "\n",
    "Usually batch normalizaition is added before activation function, after linear layer.\n",
    "\n",
    "$y = Activation(BatchNorm(WX))$\n",
    "\n",
    "There is additional normalization parameter $G_i$, $B_i$ to train. This part is named normalized activation according to origional paper.\n",
    "\n",
    "## does it work?\n",
    "\n",
    "Normalized data makes gradient descent learning easier, take this view for each layer, we would like to normalize all the layer by mean and var of the whole dataset during training.\n",
    "\n",
    "But, it is never possilble to calculate mean and var for all the data, since each update requires a full loop of dataset. And the convariance adds difficiulty to normalizatin, since it might not be invertible. Thus, BN makes two assumption: 1. each dimension of data is independent. 2. the batch data is a good approximation of full data on mean and var.\n",
    "\n",
    "suppose we want to predict a of data [a, a+1, a+2] given the seq.\n",
    "\n",
    "This makes a bad case for applying batch noralization, because the mean and var of data are not the same and each dimension is not depentent.\n",
    "\n",
    "## a test experiment: \n",
    "\n",
    "To apply batch normalization to convolutional neural network. Suppose we have p*q feature map after convolution, then we hace p*q channels and they are normalized eperately?\n",
    "\n",
    "\n",
    "There is a condebase do really similar thing: https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Lesson.ipynb\n",
    "\n",
    "## reference\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : mnist.py\n",
    "# @Author: lizhen\n",
    "# @Date  : 2020/2/4\n",
    "# @Desc  : 工具类，用于解析mnist数据集\n",
    "\n",
    "import urllib.request # python3\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "# http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "# http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "# http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "\n",
    "url_base = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "key_file = {\n",
    "        'train_img':'train-images-idx3-ubyte.gz',\n",
    "        'train_label':'train-labels-idx1-ubyte.gz',\n",
    "        'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "        'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "        }\n",
    "\n",
    "dataset_dir=os.path.dirname(os.path.abspath(__file__))\n",
    "save_file=dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000;\n",
    "test_num  = 10000;\n",
    "img_dim   = (1, 28, 28)\n",
    "img_size  = 28*28;\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: 下载mnist的文件\n",
    "    :return: null\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"downloading\"+file_name+ \"...\")\n",
    "    urllib.request.urlretrieve(url_base + file_name , file_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "def download_mnist():\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for file_name in key_file.values():\n",
    "        _download(file_name);\n",
    "\n",
    "def _load_label(file_name):\n",
    "    \"\"\"\n",
    "    解析标签\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file_path = dataset_dir+'/'+ file_name\n",
    "\n",
    "    print(\"converting \"+file_name+\" to numpy Array.\")\n",
    "    with gzip.open(file_path) as f:\n",
    "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    \"\"\"\n",
    "    解析 压缩的图片\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file_path = dataset_dir +'/' + file_name\n",
    "\n",
    "    print(\"converting \"+ file_name + \"to numpy Array\")\n",
    "    with gzip.open(file_path) as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16) # 16*8=\n",
    "    data = data.reshape(-1, img_size) # N, (W*H*C)=[N,28*28*1]\n",
    "    print(\"Done\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def _convert_numpy():\n",
    "    \"\"\"\n",
    "     解析 image和label，将其转换为numpy\n",
    "    \"\"\"\n",
    "    dataset = {}\n",
    "    dataset['train_img'] = _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    \"\"\"\n",
    "    初始化mnist数据集：\n",
    "    1. 下载mnist，\n",
    "    2. 以二进制的方式读取，并转换成numpy的ndarray对象\n",
    "    3. 将转换后的ndarray 序列化\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"download mnist dataset...\")\n",
    "    download_mnist()\n",
    "    print(\"convert to numpy array...\")\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(Y):\n",
    "    T = np.zeros((Y.size,10))\n",
    "    for idx,row in enumerate(T):\n",
    "        row[Y[idx]] = 1\n",
    "    return T\n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param normalize: 将数据标准化到0.0~1.0\n",
    "    :param flatten: 是否要将数据拉伸层1D数组的形式\n",
    "    :param one_hot_label:\n",
    "    :return: (训练数据, 训练标签), (测试数据, 测试label)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "\n",
    "    with open(save_file,'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    if normalize:\n",
    "        for key in ('train_img','test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /=255.0\n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label']  = _change_one_hot_label(dataset['test_label'])\n",
    "\n",
    "    if not flatten:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1,1,28,28) # NCHW\n",
    "\n",
    "    return (dataset['train_img'],dataset['train_label']),(dataset['test_img'], dataset['test_label'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : optimizer.py\n",
    "# @Author: lizhen\n",
    "# @Date  : 2020/2/7\n",
    "# @Desc  : 对应opt\n",
    "import numpy as np\n",
    "class BaseOpts:\n",
    "    def update(self, params, grads):\n",
    "        pass\n",
    "\n",
    "class SGD(BaseOpts):\n",
    "\n",
    "    \"\"\"\n",
    "    梯度下降（Stochastic Gradient Descent）：\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum(BaseOpts):\n",
    "\n",
    "    \"\"\"\n",
    "    Momentum SGD：\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov(BaseOpts):\n",
    "    \"\"\"\n",
    "    Nesterov's Accelerated Gradient: http://arxiv.org/abs/1212.0901\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad(BaseOpts):\n",
    "\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key] # 平方和\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop(BaseOpts):\n",
    "\n",
    "    \"\"\"\n",
    "    RMSprop：\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam(BaseOpts):\n",
    "\n",
    "    \"\"\"\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : gradient.py\n",
    "# @Author: lizhen\n",
    "# @Date  : 2020/1/27\n",
    "# @Desc  : 梯度\n",
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    '''\n",
    "    数值微分，求f(x)的梯度\n",
    "    :param f:\n",
    "    :param x:\n",
    "    :return:\n",
    "    '''\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 恢复成原来的值\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : Nets.py\n",
    "# @Author: lizhen\n",
    "# @Date  : 2020/2/15\n",
    "# @Desc  : 网络层的基类\n",
    "class Net:\n",
    "    def loss(self, x, t):\n",
    "        '''\n",
    "        调用优化器opt, 计算 x 与t 之间的差距\n",
    "        :param x:\n",
    "        :param t:\n",
    "        :return:\n",
    "        '''\n",
    "        pass\n",
    "    def gradient(self, x, t):\n",
    "        pass\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        调用loss(),获取loss value\n",
    "        根据loss值，计算数值微分，\n",
    "        :param x:\n",
    "        :param t:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @File  : multi_layer_net_extend.py\n",
    "# @Author: lizhen\n",
    "# @Date  : 2020/2/4\n",
    "# @Desc  : 多层神经网络，有全连接层\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from src.common.layers import *\n",
    "from src.nets import BaseNets\n",
    "\n",
    "class MultiLayerNetExtend(BaseNets):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_size:\n",
    "        :param hidden_size_list:\n",
    "        :param output_size:\n",
    "        :param activation:\n",
    "        :param weight_init_std:\n",
    "        :param weight_decay_lambda:\n",
    "        :param use_dropout:\n",
    "        :param dropout_ration:\n",
    "        :param use_batchnorm:\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "\n",
    "        # 初始化权重\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # gen 激活\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        '''\n",
    "\n",
    "        :param weight_init_std:\n",
    "        :return:\n",
    "        '''\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        y = self.predict(x, train_flg)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, X, T):\n",
    "        Y = self.predict(X, train_flg=False)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        if T.ndim != 1 : T = np.argmax(T, axis=1)\n",
    "\n",
    "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, X, T):\n",
    "        loss_W = lambda W: self.loss(X, T, train_flg=True)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "            \n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\n",
    "                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg=True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 获取前1000个样本作为训练数据\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def __train(weight_init_std):\n",
    "    '''\n",
    "    训练\n",
    "    :param weight_init_std: 初始化权重\n",
    "    :return:\n",
    "    '''\n",
    "    # 带有bn层\n",
    "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
    "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
    "    # 没有bn层\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
    "                                weight_init_std=weight_init_std)\n",
    "    # 优化器\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    bn_train_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_cnt = 0\n",
    "    \n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "    \n",
    "        for _network in (bn_network, network):\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "    \n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            bn_train_acc_list.append(bn_train_acc)\n",
    "    \n",
    "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
    "    \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list, bn_train_acc_list\n",
    "\n",
    "\n",
    "# 3.绘图\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):#\n",
    "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = __train(w) # 训练网络\n",
    "    \n",
    "    # plt.subplot(4,4,i+1)\n",
    "    plt.title(\"W:%3f\"%(w))\n",
    "    # if i == 15:\n",
    "    #     # 绘制给各自的子图\n",
    "    #     plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
    "    #     plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "    # else:\n",
    "    #     plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "    #     plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "\n",
    "    # if i % 4:\n",
    "    #     plt.yticks([])\n",
    "    #\n",
    "    # else:\n",
    "    #     plt.ylabel(\"accuracy\")\n",
    "    # if i < 12:\n",
    "    #     plt.xticks([])\n",
    "    #\n",
    "    # else:\n",
    "    #     plt.xlabel(\"epochs\")\n",
    "    plt.plot(x, bn_train_acc_list, label=str(i)+'epoch Batch Normalization', markevery=2)\n",
    "    plt.plot(x, train_acc_list, linestyle=\"--\", label=str(i)+'epoch Normal(without BatchNorm)', markevery=2)\n",
    "\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "plt.savefig(\"test\"+str(i)+\".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPW-mOq2XAYX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
